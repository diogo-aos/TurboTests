# Implementation phases

Excellent question. With UNIX philosophy, the phases shift from **orchestration-centric** to **contract-centric**. Here's how:

## Core Insight: Data Contracts Are the Architecture

Instead of building features in phases, you build **tools in dependency order**, with clear **JSON contracts** between them.

## Revised Phases

### Phase 1: Data Contracts (The Architecture)

Define all intermediate file formats as **JSON schemas**. This is your API.

```
questions.json ← from gift2json
config.json ← from configgen
template_layout.json ← from docgen
scan_result.json ← from imagescan
scores.json ← from scorer
```

**Deliverable:** JSON schema files + documentation. No code yet.

This is crucial: the contracts define everything.

---

### Phase 2: Zero-Dependency Tools

Build tools with no external tool dependencies first. Test each independently.

**`gift2json`** (no dependencies)

- Input: `.gift` file
- Output: `questions.json`
- Test: parse sample GIFT, verify output matches schema

**`configgen`** (depends only on questions.json format)

- Input: config parameters
- Output: `config.json`
- Test: given N questions, verify distribution logic

**`report`** (depends only on scores.json format)

- Input: `scores.json`
- Output: HTML/PDF/markdown
- Test: mock scores, verify output format

---

### Phase 3: Dependent Tools (Sequential Build)

**`docgen`** (depends on questions.json + config.json)

- Input: `questions.json` + `config.json` + template
- Output: PDFs + `template_layout.json` + `question_order.json`
- Test: use Phase 2 outputs, verify layout JSON matches schema

**`imagescan`** (depends on template_layout.json)

- Input: image + `template_layout.json`
- Output: `scan_result.json`
- Test: use synthetic filled images from Phase 2 docgen output

**`scorer`** (depends on scan_result.json + question_order.json)

- Input: `scan_result.json` + `question_order.json`
- Output: `scores.json`
- Test: mock scans, verify scores

---

### Phase 4: Composition & Pipelines

Now verify tools compose correctly.

**Deliverables:**

- Shell scripts showing composition (example pipelines)
- End-to-end tests: GIFT → PDFs → synthetic fills → extraction → scores → report
- Performance profiling (where are bottlenecks?)

```bash
# Test pipeline
gift2json questions.gift > questions.json
configgen --versions 3 --students 10 > config.json
docgen questions.json config.json --output pdfs/
  # outputs: pdfs/ + template_layout.json + question_order.json

# Synthetic scan test (automated)
generate_synthetic_fills pdfs/student_*.pdf \
  --template template_layout.json \
  --output scans/

# Score
for scan in scans/*.json; do
  imagescan $scan --layout template_layout.json
done | scorer --keys question_order.json > scores.json

report scores.json --format html > report.html
```

---

### Phase 5: Robustness & Hardening

Now optimize tools individually, knowing composition works.

**Per tool:**

- Error handling (graceful degradation)
- Edge cases (empty input, malformed JSON, etc.)
- Performance optimization
- Configuration/tier knobs (from earlier discussion)

---

## Implementation Prompts (UNIX Edition)

### Phase 1 Prompt

```
Design the data contracts for a modular test creation/scanning system.

## Objective
Define JSON schemas that are the "API" between independent tools. No code yet.

## Deliverables

1. **questions.json schema**
   - Minimal example with multiple choice + short answer
   - Include all metadata needed downstream (id, text, type, options, correct_answer, etc.)

2. **config.json schema**
   - Test configuration (versions, students, distribution strategy)
   - Includes tier knobs (question types allowed, distribution strategies available, etc.)

3. **template_layout.json schema**
   - Describes test PDF layout for scanning
   - Registration marks, answer field positions, marker types
   - Example with 5 questions (multiple choice + short answer)

4. **scan_result.json schema**
   - Output from imagescan
   - One result per question: {question_id, extracted_answer, confidence}
   - Include metadata (image source, scan timestamp, markers detected)

5. **scores.json schema**
   - Scored results: per-question scores, total, grade, confidence flags
   - Include statistics (mean, std_dev) for class-level reports

6. **Documentation** (contract_specification.md)
   - Explain each schema
   - Show example end-to-end flow
   - Describe assumptions (DPI, image resolution, marker tolerance, etc.)

## Deliverable Structure
```

phase1/ ├── schemas/ │ ├── questions_schema.json │ ├── config_schema.json │ ├── template_layout_schema.json │ ├── scan_result_schema.json │ └── scores_schema.json ├── examples/ │ ├── questions_example.json │ ├── config_example.json │ ├── template_layout_example.json │ ├── scan_result_example.json │ └── scores_example.json └── contract_specification.md

### Phase 2 Prompt

```
Build independent tools: gift2json, configgen, report.

These tools have no external tool dependencies—they're pure functions.

[Include separate prompts for each tool, focusing on:]
- Validate input against schema
- Transform to output schema
- Comprehensive error handling
- Fixtures from Phase 1 examples
- No orchestration logic yet
```

### Phase 3 Prompt

```
Build dependent tools: docgen, imagescan, scorer.

[Include separate prompts for each, emphasizing:]
- Load input from Phase 1-2 outputs
- Produce output matching schema
- Handle all tier configurations
- Synthetic testing (generate fills → extract → score)
```

### Phase 4 Prompt

```
Verify composition: tools work together.

## Objective
Test end-to-end workflows without modification to individual tools.

## Deliverables

1. **Composition Examples** (shell scripts or Python orchestrator)
   - show_full_pipeline.sh: GIFT → PDFs → synthetic → scores → report
   - show_partial_pipeline.sh: questions.json only → report (mock data)

2. **End-to-End Tests**
   - Use outputs from Phase 2-3 tools as inputs
   - Verify data flows correctly
   - No mocking—use real tool outputs

3. **Synthetic Test Automation**
   - Generate filled test images programmatically
   - Run through imagescan → scorer
   - Verify round-trip accuracy

4. **Performance Baseline**
   - Measure time per tool
   - Identify bottlenecks
   - Document expectations
```

### Phase 5 Prompt

```
Harden individual tools for production use.

[Per tool:]
- Edge case handling (empty input, malformed JSON, etc.)
- Tier configuration integration (load knobs from config)
- Logging and error reporting
- Performance optimization (if needed from Phase 4 baseline)
- Documentation and examples
```

---

## Key Differences from SaaS-Oriented Approach

|SaaS Phases|UNIX Phases|
|---|---|
|Phase 1: Parsing|Phase 1: Define contracts|
|Phase 2: Generation|Phase 2: Build independent tools|
|Phase 3: Scanning|Phase 3: Build dependent tools|
|Phase 4: Scoring|Phase 4: Test composition|
|Phase 5: Integration|Phase 5: Harden & optimize|

**Why this is better for UNIX philosophy:**

1. **Contracts first** — tools are designed to compose, not bolted together
2. **Test independently** — each tool testable in isolation
3. **Reusable** — tools useful outside this system (someone could use just `gift2json` + `configgen`)
4. **Clear boundaries** — JSON contracts are explicit, not implicit
5. **Easy to swap** — replace `imagescan` with different algorithm, no other changes needed
6. **Debuggable** — inspect JSON between tools to find issues

